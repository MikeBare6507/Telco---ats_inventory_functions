import random
import logging
import psycopg2
import string
import numpy as np

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


#VARIABLES DICTIONARY
variables = [
    {"name": "eid", "act_chance": 0.3},
    {"name": "team", "act_chance": 0.20},
    {
        "name": "eid_tenure",
        "act_chance": .5,
        "var_rel_type": {
            "modes": ["unimodal", "bimodal"],
            "relations": ["positive", "negative"],
            "mode_probabilities": {
                "unimodal": 0.5,
                "bimodal": 0.5
            },
            "relation_probabilities": {
                "positive": 0.5,
                "negative": 0.5
            }
        }
    },
    {"name": "team_tenure",
    "act_chance": 0.40,
    "var_rel_type": {
        "modes": ["unimodal", "bimodal"],
        "relations": ["positive", "negative"],
        "mode_probabilities": {
            "unimodal": 0.5,
            "bimodal": 0.5
        },
        "relation_probabilities": {
            "positive": 0.5,
            "negative": 0.5
        }
    }
},
{
        "name": "mkt_cidthq",
        "act_chance": 0.0,
        "weighted_selection": True,  # Specifies weighted sampling based on mkt_ave_cidthq
},
{
    "name": "mkt_cidtocid",
    "act_chance": 0.25,
    "weighted_selection": True  # Specifies weighted sampling based on mkt_ave_cidtocid
},
{
    "name": "eid_cidtocid",
    "act_chance": 0.30,
    "weighted_selection": True,  # Specifies weighted sampling based on eid_ave_cidtocid
},
{
    "name": "sweath_fct",
    "act_chance": 1.00,
    "weighted_selection": True,  # Enables weighted sampling based on sweath_fct
}


]

# Database connection setup (replace with your own credentials)
conn = psycopg2.connect(
    host="localhost",
    database="Telco2",
    user="postgres",
    password="[pasword]"  #Replace with actual pw
)



#VARIABLES AND SAMPLING FUNCTIONS
def handle_eid_tenure(conn):
    """
    Process the 'eid_tenure' variable with weighted sampling to address distribution issues.
    """
    logging.info("Handling eid_tenure logic with weighted sampling...")

    # Determine mode and relationship based on configured probabilities
    mode = random.choices(
        ["unimodal", "bimodal"],
        weights=[
            variables[2]["var_rel_type"]["mode_probabilities"]["unimodal"],
            variables[2]["var_rel_type"]["mode_probabilities"]["bimodal"]
        ]
    )[0]
    relationship = random.choices(
        ["positive", "negative"],
        weights=[
            variables[2]["var_rel_type"]["relation_probabilities"]["positive"],
            variables[2]["var_rel_type"]["relation_probabilities"]["negative"]
        ]
    )[0]

    logging.info(f"Selected mode: {mode}, relationship: {relationship}")

    # Fetch `eid_tenure` data
    with conn.cursor() as cursor:
        cursor.execute("SELECT eid, eid_tenure FROM var_analysis;")
        results = cursor.fetchall()

    # Filter out invalid or zero eid_tenure values and synchronize `eids` and `eid_tenures`
    filtered_results = [(row[0], float(row[1])) for row in results if row[1] and float(row[1]) > 0]
    eids, eid_tenures = zip(*filtered_results) if filtered_results else ([], [])

    if not eids:
        logging.error("No valid data available for EIDs or eid_tenure.")
        return

    # Assign weights based on mode and relationship
    if mode == "unimodal":
        if relationship == "positive":
            weights = [tenure for tenure in eid_tenures]
        else:
            weights = [max(1 / tenure, 0.1) for tenure in eid_tenures]
    elif mode == "bimodal":
        weights = [
            tenure if tenure < 10 or tenure > 20 else max(0.1, 1 / tenure)
            for tenure in eid_tenures
        ]

    # Normalize weights
    total_weight = sum(weights)
    if total_weight > 0:
        weights = [w / total_weight for w in weights]
    else:
        weights = [1 / len(eids)] * len(eids)

    # Select samples based on weights
    sample_size = random.randint(100, 300)
    selected_eids = random.choices(eids, weights=weights, k=sample_size)
    logging.info(f"Selected EIDs: {selected_eids}")

    # Process selected EIDs
    for eid in selected_eids:
        logging.info(f"Processing EID: {eid}")
        cid_subset = get_related_cid_sample(conn, eid, "eid", random.randint(12, 20))
        for cid in cid_subset:
            logging.info(f"Processing CID for eid_tenure: {cid}")
            process_asset(conn, cid, cid_subset)

def get_sample_from_query(conn, column, table, limit, condition=None):
    logging.debug(f"Preparing query to fetch {column} from {table} with limit {limit} and condition: {condition}")
    
    with conn.cursor() as cur:
        query = f"SELECT {column} FROM (SELECT DISTINCT {column} FROM {table} "
        if condition:
            query += f"WHERE {condition} "
        query += f") AS distinct_query ORDER BY RANDOM() LIMIT %s"
        cur.execute(query, (limit,))
        result = cur.fetchall()
        logging.info(f"Query results for {column}: {result}")
        return [row[0] for row in result]

def get_related_cid_sample(conn, identifier, identifier_type, sample_size):
    """
    Fetch a sample of related CIDs for a given identifier (mkt_id, eid, or cid).
    
    Parameters:
    conn (psycopg2.connection): Database connection object
    identifier (int): The identifier (mkt_id, eid, or cid) to use for fetching related CIDs
    identifier_type (str): The type of identifier, either "mkt_id", "eid", or "cid"
    sample_size (int): The desired sample size of related CIDs to fetch
    
    Returns:
    list: A list of related CID values
    """
    logging.debug(f"Fetching related CIDs for {identifier_type}={identifier} with sample size={sample_size}")
    
    if identifier_type == "mkt_id":
        query = "SELECT cid FROM var_analysis WHERE mkt_id = %s LIMIT %s"
        params = (identifier, sample_size)
    elif identifier_type == "eid":
        query = "SELECT cid FROM var_analysis WHERE eid = %s LIMIT %s"
        params = (identifier, sample_size)
    elif identifier_type == "cid":
        query = "SELECT cid FROM var_analysis LIMIT %s"
        params = (sample_size,)  # Only one parameter
    else:
        logging.error(f"Invalid identifier type: {identifier_type}")
        return []
    
    with conn.cursor() as cursor:
        cursor.execute(query, params)
        result = cursor.fetchall()
        logging.info(f"Fetched CIDs: {result}")
        return [row[0] for row in result]


def handle_team_tenure(conn):
    """
    Process the 'team_tenure' variable, selecting samples based on specified modes and relationships,
    with weighted sampling for more natural distribution shaping.
    """
    logging.info("Handling team_tenure logic...")

    # Select mode (unimodal or bimodal) based on defined probabilities
    mode = random.choices(
        ["unimodal", "bimodal"],
        weights=[variables[3]["var_rel_type"]["mode_probabilities"]["unimodal"],
                 variables[3]["var_rel_type"]["mode_probabilities"]["bimodal"]]
    )[0]
    logging.info(f"Selected mode for team_tenure: {mode}")

    # Retrieve market data and weights from the database
    try:
        with conn.cursor() as cursor:
            cursor.execute("SELECT mkt_id, mkt_ave_tenure FROM var_analysis;")
            results = cursor.fetchall()

        # Extract market IDs and tenure values
        mkt_ids = [row[0] for row in results]
        mkt_tenures = [row[1] for row in results]

        # Generate weights for tenure values
        weights_positive = [max(tenure - 8, 0) for tenure in mkt_tenures]
        weights_negative = [max(8 - tenure, 0) for tenure in mkt_tenures]

    except psycopg2.Error as e:
        logging.error(f"Database error while fetching team_tenure data: {e}")
        return

    if mode == "unimodal":
        # Select relationship type (positive or negative) within unimodal mode
        relationship = random.choices(
            ["positive", "negative"],
            weights=[variables[3]["var_rel_type"]["relation_probabilities"]["positive"],
                     variables[3]["var_rel_type"]["relation_probabilities"]["negative"]]
        )[0]
        logging.info(f"Fetching sample for {relationship} relationship in {mode} mode")

        # Apply weights based on the selected relationship
        weights = weights_positive if relationship == "positive" else weights_negative

        # Generate a weighted sample
        sample_size = random.randint(8, 15)
        if sum(weights) > 0:
            mkt_id_sample = random.choices(mkt_ids, weights=weights, k=sample_size)
        else:
            logging.warning("All weights are zero; sampling without weights.")
            mkt_id_sample = random.sample(mkt_ids, k=sample_size)

        logging.info(f"Selected mkt_id sample for {relationship} team tenure: {mkt_id_sample}")

        # Process the sampled market IDs
        for mkt_id in mkt_id_sample:
            process_team_inventory(conn, mkt_id)

    elif mode == "bimodal":
        logging.info("Fetching sample for bimodal mode (positive and negative splits)")

        # Generate weighted samples for positive and negative relationships
        sample_size = random.randint(4, 16)
        if sum(weights_positive) > 0:
            positive_sample = random.choices(mkt_ids, weights=weights_positive, k=sample_size)
        else:
            positive_sample = random.sample(mkt_ids, k=sample_size)

        if sum(weights_negative) > 0:
            negative_sample = random.choices(mkt_ids, weights=weights_negative, k=sample_size)
        else:
            negative_sample = random.sample(mkt_ids, k=sample_size)

        logging.info(f"Selected Positive team tenure mkt_ids: {positive_sample}")
        logging.info(f"Selected Negative team tenure mkt_ids: {negative_sample}")

        # Process each mkt_id within the positive and negative samples
        for mkt_id in positive_sample + negative_sample:
            logging.info(f"Processing mkt_id: {mkt_id}")
            process_team_inventory(conn, mkt_id)

def process_team_inventory(conn, mkt_id):
    """Processes CID subset associated with a given mkt_id to model team tenure effects."""
    cids_to_process = get_related_cid_sample(conn, mkt_id, "mkt_id", random.randint(5, 14))
    logging.info(f"Processing CID subset for mkt_id {mkt_id}: {cids_to_process}")

    # Perform asset adjustments (or removals) based on CID and team tenure logic
    for cid in cids_to_process:
        logging.info(f"Processing CID: {cid}")
        process_asset(conn, cid, cids_to_process)  # Use the generalized process_asset function



def handle_mkt_cidthq(conn):
    """Handles market selection based on mkt_cidthq and processes inventory for selected CIDs."""
    logging.info("Handling mkt_cidthq logic...")

    # Step 1: Select market IDs based on weighted probability of mkt_ave_cidthq
    mkt_id_sample = sample_market_ids_with_weights_cidthq(conn)
    logging.debug(f"Sampled market IDs: {mkt_id_sample}")

    # Step 2: For each selected market, process CIDs in the sample
    for mkt_id in mkt_id_sample:
        logging.info(f"Processing market ID: {mkt_id}")

        # Sample a subset of CIDs within this market
        cid_sample = get_related_cid_sample(conn, mkt_id, "mkt_id", random.randint(175, 600))
        logging.debug(f"Sampled CIDs for market {mkt_id}: {cid_sample}")
        
        # Process assets for each CID in the sample
        for cid in cid_sample:
            logging.info(f"Processing asset for CID: {cid}")
            process_asset(conn, cid, cid_sample)

def sample_market_ids_with_weights_cidthq(conn): 
    """
    Retrieves a sample of market IDs weighted by mkt_ave_cidthq values.

    Parameters:
    conn (psycopg2.connection): Database connection.

    Returns:
    list: A weighted sample of market IDs.
    """
    try:
        with conn.cursor() as cursor:
            # Retrieve mkt_id and mkt_ave_cidthq values from the database
            cursor.execute("SELECT mkt_id, mkt_ave_cidthq FROM var_analysis;")
            results = cursor.fetchall()

        # Separate mkt_id and weights
        mkt_ids = [row[0] for row in results]
        weights = [max(row[1], 0) for row in results]  # Ensure weights are non-negative

        # Generate a random sample size between 200 and 500
        sample_size = random.randint(200, 500)

        # Check if all weights are zero
        if sum(weights) == 0:
            logging.warning("All weights are zero; sampling without weights.")
            selected_mkt_ids = random.sample(mkt_ids, k=sample_size)
        else:
            selected_mkt_ids = random.choices(mkt_ids, weights=weights, k=sample_size)
        
        logging.info(f"Selected market IDs based on weighted probabilities: {selected_mkt_ids}")
        return selected_mkt_ids

    except psycopg2.Error as e:
        logging.error(f"Database error: {e}")
        return []


def handle_mkt_cidtocid(conn):
    """Handles market selection based on mkt_cidtocid and processes inventory for selected CIDs."""
    logging.info("Handling mkt_cidtocid logic...")

    # Step 1: Select market IDs based on weighted probability of mkt_ave_cidtocid
    mkt_id_sample = sample_market_cids_with_weights_cidtocid(conn)
    logging.debug(f"Sampled market IDs: {mkt_id_sample}")

    # Step 2: For each selected market, process CIDs in the sample
    for mkt_id in mkt_id_sample:
        logging.info(f"Processing market ID: {mkt_id}")

        # Sample a subset of CIDs within this market
        cid_sample = get_related_cid_sample(conn, mkt_id, "mkt_id", random.randint(140, 400))
        logging.debug(f"Sampled CIDs for market {mkt_id}: {cid_sample}")
        
        # Process assets for each CID in the sample
        for cid in cid_sample:
            logging.info(f"Processing asset for CID: {cid}")
            process_asset(conn, cid, cid_sample)

def sample_market_cids_with_weights_cidtocid(conn):
    """
    Retrieves a sample of market IDs weighted by mkt_ave_cidtocid values.

    Parameters:
    conn (psycopg2.connection): Database connection.

    Returns:
    list: A weighted sample of market IDs.
    """
    try:
        with conn.cursor() as cursor:
            # Retrieve mkt_id and mkt_ave_cidtocid values from the database
            cursor.execute("SELECT mkt_id, mkt_ave_cidtocid FROM var_analysis;")
            results = cursor.fetchall()

        # Separate mkt_id and weights
        mkt_ids = [row[0] for row in results]
        weights = [max(row[1], 0) for row in results]  # Ensure weights are non-negative

        # Generate a random sample size between 100 and 450
        sample_size = random.randint(20, 100)

        # Select market IDs based on weighted probability
        if sum(weights) == 0:
            logging.warning("All weights are zero; sampling without weights.")
            selected_mkt_ids = random.sample(mkt_ids, k=sample_size)
        else:
            selected_mkt_ids = random.choices(mkt_ids, weights=weights, k=sample_size)

        logging.info(f"Selected market IDs based on weighted probabilities: {selected_mkt_ids}")
        return selected_mkt_ids

    except psycopg2.Error as e:
        logging.error(f"Database error: {e}")
        return []


def handle_eid_cidtocid(conn):
    """Handles selection based on eid_cidtocid and processes inventory for selected CIDs."""
    logging.info("Handling eid_cidtocid logic...")

    # Step 1: Select EIDs based on weighted probability of eid_ave_cidtocid
    eid_sample = sample_eids_with_weights(conn)
    logging.debug(f"Sampled EIDs: {eid_sample}")

    # Step 2: For each selected EID, process CIDs in the sample
    for eid in eid_sample:
        logging.info(f"Processing eid: {eid}")

        # Sample a subset of CIDs assigned to this eid
        cids_to_process = get_related_cid_sample(conn, eid, "eid", random.randint(10, 20))
        logging.debug(f"Sampled CIDs for eid {eid}: {cids_to_process}")

        # Process assets for each CID in the sample
        for cid in cids_to_process:
            logging.info(f"Processing asset for CID: {cid}")
            process_asset(conn, cid, cids_to_process)

def sample_eids_with_weights(conn):
    """
    Retrieves a sample of EIDs weighted by eid_ave_cidtocid values.

    Parameters:
    conn (psycopg2.connection): Database connection.

    Returns:
    list: A weighted sample of EIDs.
    """
    try:
        with conn.cursor() as cursor:
            # Retrieve eid and eid_ave_cidtocid values from the database
            cursor.execute("SELECT eid, eid_ave_cidtocid FROM var_analysis;")
            results = cursor.fetchall()

        # Separate eids and weights
        eids = [row[0] for row in results]
        weights = [max(row[1], 0) for row in results]  # Ensure weights are non-negative

        # Generate a random sample size between 200 and 500
        sample_size = random.randint(200, 500)

        # Check if all weights are zero
        if sum(weights) == 0:
            logging.warning("All weights are zero; sampling without weights.")
            selected_eids = random.sample(eids, k=sample_size)
        else:
            selected_eids = random.choices(eids, weights=weights, k=sample_size)

        logging.info(f"Selected EIDs based on weighted probabilities: {selected_eids}")
        return selected_eids

    except psycopg2.Error as e:
        logging.error(f"Database error: {e}")
        return []


def handle_sweath_fct(conn, sample_size):
    """Handles selection based on sweath_fct and processes inventory for selected CIDs."""
    logging.info("Handling sweath_fct logic...")

    try:
        # Step 1: Select CIDs based on weighted probability of sweath_fct with a specified sample size
        cids_to_process = sample_cids_with_weights_sweath_fct(conn, sample_size=sample_size)
        logging.debug(f"Sampled CIDs: {cids_to_process}")

        # Step 2: Process each selected CID
        for cid in cids_to_process:
            logging.info(f"Processing CID for sweath_fct: {cid}")

            # Validate CID type
            if not isinstance(cid, str):
                logging.error(f"Invalid identifier type: {type(cid).__name__} for CID: {cid}")
                continue

            logging.info(f"Testing processing for CID: {cid}")
            process_asset(conn, cid, cids_to_process)  # Ensure 'process_asset' call has correct parameters
    except Exception as e:
        logging.error("Error during sweath_fct test", exc_info=True)


def sample_cids_with_weights_sweath_fct(conn, sample_size):
    """
    Retrieves a weighted sample of CIDs based on sweath_fct values.
    
    Parameters:
    conn (psycopg2.connection): Database connection.
    sample_size (int): Number of CIDs to sample.
    
    Returns:
    list: A weighted sample of CIDs.
    """
    try:
        with conn.cursor() as cursor:
            # Retrieve cid and sweath_fct values from the database
            cursor.execute("SELECT cid, sweath_fct FROM var_analysis;")
            results = cursor.fetchall()

        # Separate CIDs and weights, converting sweath_fct to float for sampling
        cids = [row[0] for row in results]
        weights = [float(row[1]) for row in results]  # Convert sweath_fct from Decimal to float

        # Select CIDs based on weighted probability
        if sum(weights) == 0:
            logging.warning("All weights are zero; sampling without weights.")
            selected_cids = random.sample(cids, k=sample_size)
        else:
            selected_cids = random.choices(cids, weights=weights, k=sample_size)

        logging.info(f"Selected CIDs based on weighted probabilities: {selected_cids}")
        return selected_cids

    except psycopg2.Error as e:
        logging.error(f"Database error: {e}")
        return []



# ASSET MANIPULATION FUNCTIONS
def process_asset(conn, cid, cids_to_process):
    # Select a random number of assets to process (between 1 and 5) for each CID
    num_assets = random.randint(2, 15)
    logging.info(f"Processing {num_assets} assets for CID: {cid}")

    # Check the initial count of assets for this cid
    asset_count = get_asset_count(conn, cid)

    for _ in range(num_assets):
        # Update asset count to ensure the loop reflects the latest asset count after each action
        if asset_count <= 1:
            logging.info(f"Only one asset left for CID {cid}, skipping further exchanges/removals.")
            break

        # Choose an action based on the given weights
        action = random.choices(
            ['add', 'exchange', 'remove'],
            weights=[0.6, 0.05, 0.35]
        )[0]

        if action == 'add':
            p_manu = "Placeholder_Manufacturer"  # Replace with actual manufacturer if needed
            asset_add(conn, cid, p_manu)
            asset_count += 1  # Increment asset count on add

        elif action == 'exchange':
            # Ensure there's more than one asset to allow an exchange
            if asset_count > 1:
                # Select a different CID to exchange with
                cid2 = random.choice([other_cid for other_cid in cids_to_process if other_cid != cid])
                success = asset_exchange(conn, cid, cid2)
                if success:
                    asset_count -= 1  # Decrement asset count on successful exchange
            else:
                logging.info(f"Skipping exchange for CID {cid} to retain at least one asset.")

        elif action == 'remove':
            # Ensure there's more than one asset to allow a removal
            if asset_count > 1:
                asset_remove(conn, cid)
                asset_count -= 1  # Decrement asset count on remove
            else:
                logging.info(f"Skipping remove for CID {cid} to retain at least one asset.")

def get_asset_count(conn, cid):
    """Returns the number of assets associated with a given CID."""
    with conn.cursor() as cur:
        cur.execute("SELECT COUNT(asset_id) FROM inv_output_uverse WHERE cid = %s", (cid,))
        result = cur.fetchone()
        return result[0] if result else 0

def asset_add(conn, cid, p_manu):
    logging.debug(f"Adding asset for CID {cid} and manufacturer {p_manu}...")
    
    part_type = select_part_type()
    part_no = select_part_no(conn, part_type, p_manu)
    if not part_no:
        logging.error(f"Failed to select part number for part type {part_type} and manufacturer {p_manu}.")
        return  # Exit function if no part number is found

    asset_id = generate_asset_id()
    if not asset_id:
        logging.error("Failed to generate a valid asset ID.")
        return  # Exit function if no asset ID is generated

    try:
        with conn.cursor() as cur:
            cur.execute(
                """INSERT INTO inv_output_uverse (cid, part_no, asset_id) VALUES (%s, %s, %s)""",
                (cid, part_no, asset_id)
            )
            conn.commit()
        logging.info(f"Added new asset - CID: {cid}, Part No: {part_no}, Asset ID: {asset_id}")
    except Exception as e:
        logging.error(f"Database error during asset addition: {e}")

def asset_exchange(conn, cid1, cid2): 
    """
    Exchanges a single asset between two CIDs.
    """
    logging.debug(f"Attempting to exchange one asset between CID {cid1} and CID {cid2}...")

    # Select an asset_id for the given cid1
    asset_id = select_asset_id(conn, cid1)
    if not asset_id:
        logging.warning(f"No assets available to exchange for CID {cid1}")
        return False  # Indicate no exchange was performed

    # Perform the exchange
    with conn.cursor() as cur:
        cur.execute(
            """UPDATE inv_output_uverse

               SET cid = %s 
               WHERE cid = %s AND asset_id = %s""",
            (cid2, cid1, asset_id)
        )
        conn.commit()  # Commit after the change

    logging.info(f"Exchanged asset - Asset ID: {asset_id}, moved from CID {cid1} to CID {cid2}")
    return True  # Indicate a successful exchange

def asset_remove(conn, cid):
    logging.debug(f"Removing a random asset for CID {cid}...")
    try:
        with conn.cursor() as cur:
            # Fetch all asset_ids for the given CID
            cur.execute("SELECT asset_id FROM inv_output_uverse WHERE cid = %s", (cid,))
            asset_ids = [row[0] for row in cur.fetchall()]

            if not asset_ids:
                logging.error(f"No assets found for CID {cid}. Skipping removal.")
                return  # Exit if no assets exist for the given CID

            # Randomly select an asset_id to remove
            asset_id_to_remove = random.choice(asset_ids)

            # Delete the row with the selected asset_id
            cur.execute("DELETE FROM inv_output_uverse WHERE asset_id = %s", (asset_id_to_remove,))
            conn.commit()
        
        logging.info(f"Removed asset with Asset ID: {asset_id_to_remove} for CID: {cid}")
    except Exception as e:
        logging.error(f"Error removing asset for CID {cid}: {e}")



def generate_asset_id():
    # Generate unique asset ID
    logging.debug("Generating asset ID...")
    first_two_chars = 'WW'
    six_digit_number1 = np.random.randint(100000, 999999)
    random_letter = random.choice(string.ascii_uppercase)
    five_digit_number2 = np.random.randint(10000, 99999)
    asset_id = f"{first_two_chars}{six_digit_number1}{random_letter}{five_digit_number2}"
    logging.info(f"Generated asset ID: {asset_id}")
    return asset_id

def select_part_type():
    # Select part type with weighted probabilities
    logging.debug("Selecting part type based on weighted probabilities...")
    
    part_categories = ['RF', 'Periph', 'Telco', 'Infra']
    weights = [0.91, 0.03, 0.03, 0.03]
    selected_part_type = random.choices(part_categories, weights=weights)[0]
    logging.info(f"Selected part type: {selected_part_type}")
    return selected_part_type

def select_part_no(conn, part_type, p_manu):
    
    logging.debug(f"Selecting part number for part type {part_type}...")
    
    prefix_conditions = {
        'RF': ["RFLT%", "RAFL%", "PTRF%"],
        'Periph': ["AN%", "AC%", "PO%"],
        'Telco': ["GT%"],
        'Infra': ["SLT%", "PT-S%"]
    }
    prefixes = prefix_conditions.get(part_type, ["RF%"])

    with conn.cursor() as cur:
        cur.execute("SELECT part_no FROM parts WHERE part_no LIKE ANY(%s)", (prefixes,))
        parts = [row[0] for row in cur.fetchall()]
    
    part_no = random.choice(parts) if parts else None
    if part_no:
        logging.info(f"Selected part number: {part_no}")
    else:
        logging.warning(f"No part number found for prefixes {prefixes}")
    return part_no

def select_asset_id(conn, cid):
    # Select a random asset_id associated with the specified CID
    logging.debug(f"Selecting asset_id for CID {cid}...")
    
    asset_ids = []
    with conn.cursor() as cur:
        # Use a parameterized query to avoid SQL injection
        query = "SELECT asset_id FROM inv_output_uverse WHERE cid LIKE %s"
        cur.execute(query, (f"%{cid}%",))  # Add '%' for SQL LIKE matching
        asset_ids = [row[0] for row in cur.fetchall()]
    
    
    asset_id = random.choice(asset_ids) if asset_ids else None
    if asset_id:
        logging.info(f"Selected asset_id: {asset_id}")
    else:
        logging.warning(f"No asset_id found for CID {cid}")
    
    return asset_id  # Return the selected asset_id


# MAIN FUNCTION to manage the entire process
def main():
    """
    Main process to manage all logic and handle each variable.
    """
    try:
        logging.info("Starting main process...")

        # Sample size ranges for each variable
        sample_size_params = {
            "eid": (19, 200),
            "mkt_id":(1, 10),
            "eid_tenure":(10, 20),
            "team": (8, 15),
            "team_tenure": (10, 20),
            "mkt_cidthq": (0,0),
            "mkt_cidtocid": (155, 410),
            "eid_cidtocid": (160, 422),
            "sweath_fct": (2000, 14000)
        }

       

        # Main loop for handling all variable-related logic
        for var in variables:
            # Check if the variable should be activated based on 'act_chance'
            if random.random() <= var["act_chance"]:
                logging.info(f"{var['name']} activated")
            else:
                logging.info(f"{var['name']} deactivated")
                continue  # Skip further processing for this variable if not activated

            # Look up the sample size range for the variable
            sample_size_range = sample_size_params.get(var["name"])

            # If no sample size is provided (None), skip setting sample size
            if sample_size_range is not None:
                sample_size = random.randint(*sample_size_range)
            else:
                sample_size = None

            if var["name"] == "eid":
                # Process 'eid' variable
                eid_sample = get_sample_from_query(conn, "eid", "var_analysis", sample_size or random.randint(90, 297))
                for eid in eid_sample:
                    logging.info(f"Processing EID: {eid}")
                    cid_subset = get_related_cid_sample(conn, eid, "eid", random.randint(13, 20))
                    for cid in cid_subset:
                        process_asset(conn, cid, cid_subset)

            elif var["name"] == "eid_tenure":
                # Process 'eid_tenure' variable
                logging.debug("Handling eid_tenure logic")
                handle_eid_tenure(conn)

            elif var["name"] == "team":
                # Process 'team' variable
                mkt_ids_sample = get_sample_from_query(conn, "mkt_id", "var_analysis", sample_size or random.randint(0, 5))
                for mkt_id in mkt_ids_sample:
                    logging.info(f"Processing mkt_id: {mkt_id}")
                    cid_subset = get_related_cid_sample(conn, mkt_id, "mkt_id", random.randint(45, 335))
                    for cid in cid_subset:
                        logging.info(f"Processing CID: {cid}")
                        process_asset(conn, cid, cid_subset)

            elif var["name"] == "team_tenure":
                # Process 'team_tenure' variable
                logging.info("Processing team tenure...")
                mkt_ids_sample = get_sample_from_query(conn, "mkt_id", "var_analysis", sample_size or random.randint(0, 5))
                for mkt_id in mkt_ids_sample:
                    logging.info(f"Processing mkt_id: {mkt_id}")
                    cid_subset = get_related_cid_sample(conn, mkt_id, "mkt_id", random.randint(115, 485))
                    for cid in cid_subset:
                        logging.info(f"Processing CID: {cid}")
                        process_asset(conn, cid, cid_subset)

            elif var["name"] == "mkt_cidthq":
                # Process 'mkt_cidthq' variable
                logging.debug("Handling mkt_cidthq logic")
                handle_mkt_cidthq(conn)

            elif var["name"] == "mkt_cidtocid":
                # Process 'mkt_cidtocid' variable
                logging.debug("Handling mkt_cidtocid logic")
                handle_mkt_cidtocid(conn)

            elif var["name"] == "eid_cidtocid":
                # Process 'eid_cidtocid' variable
                logging.info("Processing eid_cidtocid...")
                eids_sample = get_sample_from_query(conn, "eid", "var_analysis", sample_size or random.randint(120, 440))
                for eid in eids_sample:
                    logging.info(f"Processing eid for eid_cidtocid: {eid}")
                    cid_subset = get_related_cid_sample(conn, eid, "eid", random.randint(10, 25))
                    for cid in cid_subset:
                        logging.info(f"Processing CID for eid_cidtocid: {cid}")
                        process_asset(conn, cid, cid_subset)

            elif var["name"] == "sweath_fct":
                logging.info("Processing sweath_fct...")

                # Use the sample size from the dictionary
                sample_size_range = sample_size_params.get("sweath_fct", (4000, 16000))
                sample_size = random.randint(*sample_size_range)

                # Directly call handle_sweath_fct, which will handle the sampling internally
                handle_sweath_fct(conn, sample_size=sample_size)

    except Exception as e:
        logging.error(f"Error occurred: {e}")
    
    finally:
        logging.debug("In finally block - closing connection...")
        if conn:
            logging.debug(f"Connection status before close: {conn.closed}")
            conn.close()
            logging.info("Connection closed")

if __name__ == "__main__":
    main()
